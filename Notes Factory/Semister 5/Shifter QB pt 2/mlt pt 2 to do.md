## Chapter 3: Supervised Learning – CO2  

- [x] 20  
### k-Nearest Neighbour (kNN) – Working / ``sums`` – 4/6 Marks  

***

- [ ] 21  
### Define Information Gain and Gini Index (with formulas), Pruning – 2 Marks  

***

- [ ] 22  
### Find out Root Node using Gini Index and Information Gain for Classification (ID3 and CART) – 6/8 Marks  

***

- [ ] 23  
### Support Vector Machines – 4/6 Marks  

***

---

## Chapter 4: Unsupervised Learning – CO3 (Marks: 16)  

- [x] 1  
### Define Clustering. Give Example of Clustering – 4 Marks  


***
-   [x] 2
    

### Define Types of Clustering (Hard and Soft) – 4 Marks

***

- [x] 3  
### Explain Types of Clustering Methods – 3/4 Marks  

***

- [ ] 4  
### Formulas on Different Types of Distance Metrics with Example – 2/3/4 Marks  

***

- [ ] 5  
### K-Means Clustering – Process and s`sums` – 4/6/8 Marks  

***

- [ ] 6  
### K-Medoids – `sums`, Advantages and Disadvantages – 4/6/8 Marks  

***

- [ ] 7  
### Define Hierarchical Clustering – 2/3 Marks  

***

- [ ] 8  
### Explain Dendrogram with Graphical Representation and Example – 4 Marks  

***

- [ ] 9  
### Types of Hierarchical Clustering – 2/4 Marks  

***

- [ ] 10  
### Explain Hierarchical Agglomerative Clustering and Hierarchical Divisive Clustering with Suitable Diagram (Workflow) – 4/6 Marks  

***

- [ ] 11  
### Explain DBSCAN with Terminology (Core, Border, Noise Point, Density Edge, Density Connected Points) with Example – 2/4/6 Marks  

***

- [ ] 12  
### `sums` based on Agglomerative Clustering – 4/6/8 Marks  

***

---

## Chapter 5: Ensemble Learning – CO4 (Marks: 10)  

- [ ] 1  
### Explain Ensemble Learning – 2 Marks  

***

- [ ] 2  
### Explain Bagging – Working, Benefits, Applications with Suitable Diagram – 4/6 Marks  

***

- [ ] 3  
### Explain Boosting – Working, Benefits, Applications with Suitable Diagram – 4/6 Marks  

***

- [ ] 4  
### Write Short Note on Subbagging – 4 Marks  

***

- [ ] 5  
### Difference between Bagging and Subbagging – 4 Marks  

***

- [ ] 6  
### Explain Working of Random Forest Algorithm with Advantages and Disadvantages – 6 Marks  

***

- [ ] 7   RISK THIS IS NOT THERE IN PT 2
### `sums` based on Decision Tree Regression – 6/8 Marks  

***

- [ ] 8  
### Define Cross Validation. Importance of Cross Validation – 2/3 Marks  

***

- [ ] 9  
### Explain K-Fold Cross Validation with Example – 4/6 Marks  

***

- [ ] 10  
### Explain Leave-One-Out Cross-Validation with Example – 3 Marks  

***

- [ ] 11    RISK THIS IS NOT THERE IN PT 2
### Define Variance, Variance Reduction – 3/4 Marks  

***

- [ ] 12    RISK THIS IS NOT THERE IN PT 2
### Find out Root Node using Standard Deviation Reduction for Regression – 4/6 Marks  

***

- [ ] 13  
### Examples of Bagging and Boosting – 4 Marks  

***

- [ ] 14  
### Importance of random_state – 2 Marks  

***
